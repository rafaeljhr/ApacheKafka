The EKS console allows you to see not only the configuration aspects of your cluster, 
but also to view Kubernetes cluster objects such as Deployments, Pods, and Nodes. 
For this type of access, the console IAM User or Role needs to be granted 
permission within the cluster.

By default, the credentials used to create the cluster are automatically granted these permissions.
we have  created a cluster using temporary IAM credentials from within Cloud9. 
This means that weâ€™ll need to add our AWS Console credentials to the cluster. 

Import your EKS Console credentials to your new cluster
IAM Users and Roles are bound to an EKS Kubernetes cluster via a ConfigMap named aws-auth. 
We can use eksctl to do this with one command.
Determine the correct credential to add for your AWS Console access.

$
c9builder=$(aws cloud9 describe-environment-memberships --environment-id=$C9_PID | jq -r '.memberships[].userArn')
if echo ${c9builder} | grep -q user; then
	rolearn=${c9builder}
        echo Role ARN: ${rolearn}
elif echo ${c9builder} | grep -q assumed-role; then
        assumedrolename=$(echo ${c9builder} | awk -F/ '{print $(NF-1)}')
        rolearn=$(aws iam get-role --role-name ${assumedrolename} --query Role.Arn --output text) 
        echo Role ARN: ${rolearn}
fi

#that should show the ARN
$eksctl create iamidentitymapping --cluster ekswork-eksctl --arn ${rolearn} --group system:masters --username admin

#verify entry in auth map
kubectl describe configmap -n kube-system aws-auth

